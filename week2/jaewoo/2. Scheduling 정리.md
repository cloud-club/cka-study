
## Manual Scheduling

- pod manifest file엔 `NodeName`  항목이 있는데 이는 k8s가 자동으로 추가
- Scheduler는 모든 Pod를 살펴보고 `NodeName` 항목이 없는 pod를 찾음
- 이는 schduler 후보
- 식별된 후보들은 Schedule 알고리즘으로 인해 바인딩 될 Node를 부여(바인딩 객체를 활용하여)

스케쥴러가 없으면 pending 상태의 pod를 직접 node에 할당하는 설정을 해줘야함
포드는 최초 생성시에만 `nodeName` 항목을 적용할 수 있음

만약 직접 수정하고 싶으면 바인딩 객체를 생성하여  바인딩 api에 post 요청 보내야 한다.

**실행중인 pod를 다른 Node로 옮길 수 없다.**
	pod는 container이고 특정 system에서 돌아가는 process다.
	특정 system에서 돌아가는 process를 다른 system으로 옮길 수 는 없는 것과 같다.


## Labels & Selectors

``` yaml
apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp
	labels:
		app: App1
		function: Front-end

spec:
	containers:
	- name: simple-webapp
	  image: simple-webapp
	  ports:
	  - containerPort: 8080

```

`$ kubectl get pods --selector app=App1`
```
Name            Ready         STATUS        RESTARTS      AGE
simple-webapp     0/1       Completed            0         1d
```

k8s는 labels와 selectors를 이용해 서로 다른 object 를 연결한다.

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
	name: simple-webapp
	labels:
		app: App1 # ReplicaSet 자체의 Label
		function: Front-end
spec:
	replicas: 3
	selector:
		matchLabels:
			app: App1
	template:
		metadata:
			labels:
				app: App1
				function: Front-end
		spce:
			containers:
			- name: simple-webapp
			- image: simple-webapp
```

`spec.selector.matchLabels` 와 `spec.template.metadata.labels` 가 일치 해야 `ReplicaSet`이 Pod를 관리한다. 

또한 `ReplicaSet`의 `metadata.labels`는 `spec.selector.matchLabels`와 직접적인 연관이 없다.

즉 `ReplicaSet`의 자체 Lable(`metadata.lables)은 ` `Pod` 를 찾는 기준이 아니다.
이건 `ReplicaSet`을 식별하는 용도이다.
즉 `kubectl get replicaset -l team=DevOps` 같은 ,..

```
📌 ReplicaSet (team=DevOps)
 ├── Selector (matchLabels: app=App1)
 ├── Pod 1 (app=App1, env=production) ✅
 ├── Pod 2 (app=App1, env=production) ✅
 ├── Pod 3 (app=App2, env=production) ❌ (다른 app 값)

```


## Taints And Tolerations

Taint는 얼룩이라는 뜻으로 Node에 설정.
	- Node에 Taint를 설정하여 임의의 Pod가 할당되는 것을 방지한다.

Toleration은 용인 이라는 뜻으로 Pod에 설정한다.
	- 특정 Taint를 용인할 수 있는 Taleration 설정을 가진 Pod는 해당 Node에 할당 가능하다.
	- 즉, 이 2가지 전략을 동시에 사용하는 것.


### 관리법

- Node를 새로 구성할 때 kubelet 옵션을 통해 기본 Taint 설정 가능
- 이미 실행 중인 Node에 대해 kubectl을 통해 Node의 Taint 관리 가능
	- Taint는 Label 및 Annotation과 비슷하게 Key=Value 형식을 가지지만, 추가적으로 Effect라는 파라미터를 가진다.

- Effect
	- Taint가 Node에 설정될 때 어떤 효과를 가지게 될지 설정하는 것이다.
	- 3가지
		- NoSchedule
			- 그 Taint Node에 Pod의 스케줄링을 허용하지 않음
			- 기존 실행 중인 Pod는 어쩔 수 없고, 앞으로 실행될 Pod에 대해서만 스케쥴링 제한
		- NoExecute
			- 그 Taint Node에 Pod의 실행을 허용하지 않음
			- 앞으로 생성될 Pod에 대한 스케쥴링을 제한함은 물론, 기존에 해당 Node에 배치된 Pod 모두 방출
		- PreferNoSchedule
			- 그 Taint Node에 Pod 스케쥴링을 선호하지 않음
			- Soft룰임
			- 기존 실행중인 Pod는 허용하고, 앞으로 생성될 Pod도 스케쥴링 되는 것을 선호하진 않지만, 해당 Noe 밖에 스케쥴링 될 곳이 없다면 허용해줌

## Node Selectors

![[Pasted image 20250316105700.png | 200]]
- 다음과 같이 리소스의 차이가 있는 Node들이 있다.
- 큰 리소스가 필요한 Pod가 Node1에 들어가야 할 것이다.
- 기본 설정 상으론 이게 불가능 


![[Pasted image 20250316110000.png | 500]]

다음과 같이 nodeSelector을 사용하여 적절한 Node를 찾아낸다.
이렇게 설정하려면 Node에 Label하는 과정이 선행되어야 한다.
`kubectl label nodes <node-name> <label-key>=<label-value>`

단순하지만 한계가있다.
- Large or Medium
- NOT Small
이런 디테일한 설정이 불가능하다.


## Node Affinity

NodeSelector 의 고급 기능.
Node Selector과 비슷하게 노드의 Label을 기반으로 Pod를 Scheduling 한다.

3가지 필드가 있다.

- requiredDuringSchedulingIgnoredDuringExecution : 스케쥴링 하는 동안 꼭 필요한 조건
- preferredDuringSchedulingIgnoredDuringExecution: 스키쥴링 하는 동안 만족하면 좋은 조건

두 필드 모두 실행중인 Pod에는 조건이 바뀌어도 무시한다. 즉 Pod가 이미 스케쥴링되어 특정 Node에서 실행 중 이라면 해당 Node의 조건이 변경되더라고 실행중인 Pod는 그대로 실행된다.

- requiredDuringSchedulingRequiredDuringExecution

 이는 만약 Node의 Label 조건이 바뀌게 되고 해당 Pod의 Affinity가 그 조건을 만족하지 못하면 해당 Pod는 퇴출된다.

### requiredDuringSchedulingIgnoredDuringExecution
```yaml
apiVersion:
kind:

metadata:
	name: myapp-pod
spec:
	containers:
	- name: data-processor
	- image: data-processor
	affinity:
		nodeAffinity:
		requiredDuringSchedulingIgnoredDuringExecution:
			nodeSelectorTerms:
			- matchExpressions:
				- key: size
				  operator: In
				  values:
				  - Large
				  - Medium
```

다음과 같이 Large, Medium 동시에 넣을 수도 있고

```yaml
...생략
operatort: NotIn
values:
- Small
```

다음과 같이 NotIn으로 제외 시킬 수도 있다.

| 조건               | 설명                                                                                                |
| ---------------- | ------------------------------------------------------------------------------------------------- |
| **In**           | `values[]` 필드에 설정한 값 중 레이블에 있는 값과 일치하는 것이 하나라도 있는지 확인합니다.                                         |
| **NotIn**        | `In`과 반대로 `values[]`에 있는 값 모두와 맞지 않는지 확인합니다.                                                      |
| **Exists**       | `key` 필드에 설정한 값이 레이블에 있는지만 확인합니다. (`values[]` 필드가 필요 없습니다.)                                       |
| **DoesNotExist** | `Exists`와 반대로 노드의 레이블에 `key` 필드 값이 없는지만 확인합니다.                                                    |
| **Gt**           | Greater than의 약자로, `values[]` 필드에 설정된 값보다 더 큰 숫자형 데이터인지 확인합니다. 이때 `values[]` 필드에는 값이 하나만 있어야 합니다. |
| **Lt**           | Lower than의 약자로, `values[]` 필드에 설정된 값보다 더 작은 숫자형 데이터인지 확인합니다. 이때 `values[]` 필드에는 값이 하나만 있어야 합니다.  |

### preferredDuringSchedulingIgnoredDuringExecution

```
...생략
preferredDuringSchedulingIgnoredDuringExecution:
- weight: 10
  preference:
  - matchExpressions:
    - key: disktype
      operator: In
      values:
      - hdd

```


preference 필드는 해당 조건에 맞는 걸 선호 한다는 뜻. 조건 맞는 Node를 우선 선택하지만 없다면 없는대로 조건에 맞지않는 Node에 Pod를 스케쥴링한다.
- `weight`는 1~100까지 설정 가능
- 여러개의 `matchExpressions[]` 필드 안 설정 각각이 노드의 설정과 맞을 때 마다 `weight` 필드 값을 더한다. 그리고 모든 노드 중 `weight` 값의 합계가 가장 큰 Node를 선택한다.


## Node Affinity vs Taints and Tolerations

이 두 가지 규칙을 적절히 조합하여 정확히 원하는 Pod를 Node에 할당할 수 있다.

## Resource Requirements and limits

```yaml
...생략
spec:
	resources:
		requests:
			memory: "4Gi"
			cpu: 2
		limits:
			memory: "2Gi"
			cpu: 2
```

pod 설정에서 설정해 두면 해당 requests 가 보장된 node에 스케쥴링된다. 

### Request

요청을 하게 되면 스케쥴러는 요청한 만큼 사용할 수 있도록 보장해준다.
Pod가 요청한 양을 모두 사용하던 적게 사용하던 상관 없이
kubelet은 요청한 만큼 용량을 확보해서 Pod에게 제공한다.

### Limit

만약 1G를 요청했고 2G로 제한을 두었을 때 1G는 확보해서 무조건 할당해 주고 최대 2G까지 늘려줄 수 있다.
하지만 (제한-요청)만큼의 용량을 미리 확보해 두지 않는다.
최대 2G까지 사용할 수 있다는 것이지 실제로 그 용량을 사용할 수 있는지에 대한 여부는 다른 Pod의 요청량과 연관이 있다.

Pod를 describe 해보면
```
Qos Class: BestEffort
```
다음과 같은 항목이 있다.
- `BestEffort`: 가장 나쁨 : 요청/제한 설정이 되어 있지 않음
- `Burstable`: 요청 < 제한
- `Guaranteed`: 가장 좋음 : 요청 = 제한

특정 Node에 여러 Pod가 띄워져 있으면 cpu, memory 자원을 경합하게 된다.

경합 시 우선순위는 `Guaranteed`, `Burstable`, `BestEffort` 순이다.

그래서 실제로 `BestEffort`는 리소스를 아예 할당 받지 못하는 경우도 있다.
또한 RS가 있을 때 삭제되는 첫 번째 대상 또한 `BestEffort`이다.
즉, 리소스가 없어서 누군가 죽어야 한다면 그 대상은 `BestEffort`이다.

반대로 `Guaranteed`는 항상 실행이 보장된다.
그러나 이 또한 너무 많으면 가장 최근 생성분은 안된다.

CPU는 limit 이상의 리소스를 사용하지 못하도록 제한함
Memory는 limit 이상의 리소스를 사용할 수 있음
그러나 limit 이상으로 사용하면 oom을 뱉으며 terminate 됨

### 단위

CPU : 
- milicore(m) 단위 사용
- 1500m -> cpu 1.5개, 1000m -> cpu 1개 (정확히는 core 1개)
Memory :
- M, G, T, Mi, Gi, Ti

### ResourceQuota

ResourceQuota는 ns별 총 resource 사용을 제한하는 조건을 제공. 특정 ns가 resource를 과하게 사용할 수 있음을 방지.

### LimitRange

ns 내에서 Pod나 Container은 ns내의 resource quota에 정의된 만큼 CPU와 Memory를 사용할 수 있는데, 이 때 하나의 Pod 또는 Container가 사용가능한 모든 Resource를 독점할 수 있는 우려가 있음
LimitRange는 ns에서 움직이는 Pod 하나한의 Resource 상한을 설정

![[Pasted image 20250316133810.png | 500]]

## Kubernetes Pod 및 Deployment 편집

### Pod 편집

- 기존 Pod의 일부 필드만 수정 가능
    - `spec.containers[*].image`
    - `spec.initContainers[*].image`
    - `spec.activeDeadlineSeconds`
    - `spec.tolerations`
- 환경 변수, 서비스 계정, 리소스 제한 등의 변경 불가
- Pod을 변경하려면 두 가지 방법 사용 가능
    1. `kubectl edit pod <pod-name>` 실행 후 저장 시 변경 불가 → 임시 파일 생성됨  
        → 기존 Pod 삭제 후(`kubectl delete pod <pod-name>`) 임시 파일로 새 Pod 생성 (`kubectl create -f /tmp/kubectl-edit-xxx.yaml`)
    2. `kubectl get pod <pod-name> -o yaml > my-new-pod.yaml` 로 YAML 추출  
        → 수정 후 기존 Pod 삭제 및 새로운 Pod 생성

### Deployment 편집

- `kubectl edit deployment <deployment-name>` 사용하여 **POD 템플릿의 모든 필드 수정 가능**
- 변경 사항 적용 시 기존 Pod 삭제 후 새로운 Pod 자동 생성됨


## DaemonSets

- Cluster 전체에서 포드를 띄울 때 사용하는 컨트롤러다.
- 일부 혹은 모든 노드에 동일한 Pod가 하나씩 실행되도록 보장한다.
- 클러스터에 새 노드가 추가될 때 마다 Pod 복제본이 자동으로 해당 Node에 추가됨.
- Node가 제거되면 Pod는 Garbage Collection의 대상이 되어 자동으로 제거됨

Deployment는 롤링 업데이트, 배포 일시 중지 등 배포 작업을 세분화 한다면
Daemonset은 특정 Node, 또는 모든 Node에 실행되어야 할 특정 Pod를 관리

로그 수집기나, 모니터링 Pod를 실행할 때 사용

꼭 모든 Cluster에 띄워야 하는건 아니고, Taint, Toleration 옵션을 활용해 특정 Node에 띄울 수 있다.

`k create daemonset` 이 없기 때문에 유사한 Deployment로 yaml 생성하고 편집하자.


## Static Pods

일반적으로 Pod는 Deployment, StatefulSet, DaemonSet 등의 컨트롤러를 통해 생성, 관리된다.

Static Pod는 API Server의 관여 없이 특정 Nodedptj kubelet 데몬에 의해 직접 관리된다.\
controlPlane에 의해 관리되는 일반 Pod와 달리, kubelet이 각 Static Pod를 관찰하고 실패하면 재실행 시킨다.

### Regular Pod

일반 Pod는 API 서버에 의해 관리되고 관리 데이터는 etcd에 저장된다.
따라서 kubectl 같은 명령어를 통해 쉽게 조회, 수정, 삭제 가능하다.

일반 Pod의 경우, kubelet은 kube-apiserver에 의존하여 해당 kubelet이 위치한 노드에 어떤 Pod를 배치할지 지시를 받는다. 이 지시는 kube-scheduler의 판단에 따라 결정되고, 이후 데이터 저장소에 저장된다.

애플리케이션 배포, 스케일링, 업데이트 등 일반적인 k8s 작업에 사용된다.

### Static Pod

Static Pod는 API 서버와 독집적으로 동작한다.
kubelet은 Static Pod를 감지하고 생성하지만, static pod의 정보를 따로 저장하지 않는다.

대신 API 서버는 "Mirror Pod" 형태로 조회할 수 있다.
kubelet은 각 Static Pod에 해당하는 Mirror Pod를 kubernetes API에 생성을 시도한다.

그래서 etcd에 저장되어 있지 않더라고 kube-apiserver API나 kubelet 명령어를 통해 static pod 정보를 확인할 수 있다.
즉, kube-apiserver에서 조회는 가능해도 mirror 객체이기 때문에 제어는 불가능하다.

주로 Cluster 부팅 과정에 필수적인 컴포넌트를 실행할 때 사용된다.
etcd나 kube-apiserver등과 같은..


### How to work?

kubelet 데몬으로 실행되기 때문에 노드를 독립적으로 관리할 수 있다.

그러나 Pod 상세 data를 제공해줄 API 서버가 없다.

![[Pasted image 20250316161118.png | 500]]

Pod에 관한 정보를 저장하는 서버 디렉터리를 관리하고 해당 file을 읽는다.

kubelet은 주기적으로 해당 디렉터리 하위의 파일을 읽고 Pod를 생성하여 Pod가 죽지 않도록 보장한다.
Pod가 고장나면 kubelet이 재시작을 시도하며 디렉터리 내 파일이 변경되면 kubelet이 Pod를 재생성 하여 변경사항을 적용한다.
파일을 제거하면 해당 Static Pod가 자동으로 삭제된다.

API 서버나 k8s 클러스터 구성 요소의 간섭 없이 kubelet이 스스로 만든 Pod이다.

replicaset, deployment, service 등을 통해 배치될 수 없다.

위의 설명한 디렉터리는 `/etc/kubernetes/manifests` 이다.


## Multiple Schedulers

- 기본 Scheduler는 다양한 알고리즘을 통해 Node에 Pod를 고르게 분배한다.
- 이 과정에서 Taint, Toleration, Node Affinity 등의 조건을 고려한다.
- 그러나 특정 애플리케이션에서는 이 기본 Scheduler가 적합하지  않을 수 있다.
- 이런 경우 고유한 Schedul 알고리즘을 적용할 수 있다.

k8s Cluster는 여러 Scheduler를 동시에 사용할 수 있고, 각 Scheduler는 서로 다른 이름을 가져야 한다.
기본 Scheduler 이름은 kube-scheduler이다.

### 배포 방법

1.  바이너리 파일 직접 실행
   
```terminal
$ wget https: ~~~~~~
$ ExecStart=/usr/local/bin/kube-scheduler \\
	--config=/etc/kubernetes/config/my-scheduler.yaml
```
대부분의 경우 이렇게 배포 하지 않는다. pod 나 depolyment로 배포한다.

2.  Deploy as Pod
   


**my-custom-scheduler.yaml** (스케줄러 Pod 정의 파일)
```yaml
kind: Pod
metadata:
	name: my-custom-scheduler
	namespace: kube-system
spec:
	containers:
	- command:
	  - kube-scheduler
	  - --address=127.0.0.1
	  - --kubeconfig=/etc/kubernetes/scheduler.conf
	  - --config=/etc/kubernetes/my-scheduler-config.yaml

	  image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
	  name: kube-scheduler
```

**my-scheduler-config.yaml** (스케줄러 설정 파일)
```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: kubeSchedulerConfiguration
profiles:
- schedulerName: my-scheduler
leaderElection:
	leaderElect: true
	resourceNamespace: kube-system
	resourceName: lock-object-my-scheduler
```

- `my-custom-scheduler.yaml`은 커스텀 스케줄러 Pod를 실행하며, 실행시 `--config` 옵션을 통해 설정을 적용함
- `my-scheduler-config.yaml`에서 `schedulerName: my-scheduler`를 지정했기 때문에 `spec.schedulerName: my-scheduler`를 설정한 Pod들은 기본 스케줄러 대신 이 커스텀 스케줄러에서 스케줄링 됨
- `LeaderElection` 설정은 k8s에서 다중 스케줄러가 실행될 때 충돌 방지 역할임.

3. Deploy as Depolyment 

### view Event

`kubectl get events -o wide`
통해 Schedule의 정상 작동 여부 확인 가능.

### 필요 권한 설정
#### ServiceAccount
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-scheduler
  namespace: kube-system

```
- `my-scheduler` 라는 ServiceAccount 가 `kube-system` 네임스페이스에 생성됨.
- Kubernetes에서 Pod가 API 서버와 상호작용할 때 사용할 인증 정보(토큰)를 제공.
- 기본적으로 모든 Pod는 `default` ServiceAccount를 사용하지만, 커스텀 스케줄러는 특별한 권한이 필요하므로 별도의 ServiceAccount를 사용함.

#### ClusterRoleBinding
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-scheduler-as-kube-scheduler
subjects:
- kind: ServiceAccount
  name: my-scheduler
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
```
- `my-scheduler` ServiceAccount가 k8s 기본 스케줄러(kube-scheduler)와 동일한 권한을 갖도록 설정.
- `roleRef.name: system:kube-scheduler` -> k8s 기본 스케줄러에 필요한 권한을 가진 CluserRole을 바인딩
- 

#### ClusterRoleBinding
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-scheduler-as-volume-scheduler
subjects:
- kind: ServiceAccount
  name: my-scheduler
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:volume-scheduler
  apiGroup: rbac.authorization.k8s.io
```
- 스케줄러가 Persistent Volume 관련 리소스를 다룰 수 있도록 추가 권한 부여
- roleRef.name: system:volume-scheduler -> 볼륨 스케줄링 관련 권한을 가진 `ClusterRole`과 연결

#### ???
- k8s 스케줄러는 API 서버에 접근하여 Pod를 배치할 Node를 결정하는 역할을 한다.
- 기본 `kube-scheduler`는 `system:kube-scheduler` 역할을 가지며, API 서버에서 필요한 정보를 가져올 수 있다.
- 커스텀 스케줄러도 동일한 기능을 수행해야 하므로, 같은 권한을 부여해야 한다.
- 또한, Persistent Volume 관련 스케줄링을 처리하려면 `system:volume-scheduler` 권한도 필요



## Scheduler Profile

- Scheduler는 Worker Node에서 k8s Pod의 스케줄링을 담당한다.
- 한 Pod를 배포할 때 CPU, Memory, Affinity, Taint and Tolerations, Priority, PV 등 과 같은 Pod 요구 사항을 지정한다.
- 스케줄러의 주된 역할은 생성된 요청을 식별하고 요구 사항을 충족하는 Pod가 배치될 가장 적합한 Node를 선택하는 것.

스케줄러는 `Scheduling cycle`과 `Binding cycle` 두 단계로 되어 있고, 이를 합쳐 `scheduling context`라고 한다.
`Scheduling cycle`은 최적의 노드를 선택하기 위해, filtering과 scoring 작업을 수행하고, `Binding cycle`은 binding 이벤트를 생성하여 그 변화를 Cluster에 적용한다.


#### 1. Scheduling Queue


가장 먼저 Pod가 생성되면 Pod는 `Scheduling Queue` 끝에 추가되어, 스케줄링 될 때까지 대기한다.
스케줄러는 스케줄링을 위해 항상 우선순위가 높은 Pod들을 우선순위가 낮은 Pod들보다 먼저 배치한다.

이 때, 큐 내의 Pod들은 Priority에 따라 정렬된다.
이 Priority는 아래의 PriorityClass 객체를 정의해서 Pod에 적용할 수 있다.
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
```
이 큐에 순서대로 Pod를 스케줄 할 수 있는 가장 적합한 Node를 찾기 위한 다음 단계로 넘어간다.

**plugIn**
- PrioritySort

#### 2. Filtering

Filtering 단계에서는, Pod를 실행할 수 없는 Node를 거른다. 
즉, 리소스가 부족하여 Pod를 실행시킬 수 없는 Node를 Filtering 하는 단계이다.

**plugIn**
- NodeResourcesFit - 리소스보다 부족한 용량의 Node를 후보에서 제외
- NodeName - pod spec에 스케줄링 할 Node가 이미 선정되어 있으면 해당 Node에 Pod를 스케줄링
- NodeUnschedulable - 스케줄링이 되면 안될 Node 제외

#### 3. Scoring

- 스케줄러가 필터링된 워커 Node들에 점수를 부여하여 Node 순위를 매긴다.
- 그 중, 가장 높은 순위를 가지는 Node가 Pod 스케줄링을 위해 선택된다. 동일하다면 랜덤.
- 스케줄러는 여러 스케줄링 플러그인을 활용하여 스코어링을 한다.
- 기본적으로, 각 Node에 해당 Pod에 필요한 CPU를 사용한 후 남은 공간을 수치로 점수를 매긴다.

**plugIn**
- NodeResourcesFit - resource가 많은 node가 점수가 높다.
- ImageLocality - pod의 image를 가지고 있는 Node가 점수가 높다.
#### 4. Binding

Node가 선택되면 스케줄러는 API 서버에 Binding 이벤트를 생성하여 Binding 시킨다.

**plugIn**
- DefaultBinder

이런 4개의 Step 말고 다음과 같이 더 많은 Step이 존재한다.

![[Pasted image 20250316232633.png | 700]]


- 이러한 작업들의 각 단계는 PlugIn으로 작동된다.
- k8s는 이러한 상황에서 어떤 PlugIn을 어디에 둘지 등의 커스터마이징을 할 수 있고 이는 `Extension Points`를 통해 구현된다.
- 즉, KubeSchedulerConfiguration에서 Scheduler마다 사용자가 원하는 방식으로 Step별 plugIn을 disable시키거나 enable 시킬 때 사용한다.

![[Pasted image 20250316232832.png | 700]]
다음 그림을 보면 my-2 에서는 score 단에서 사용할 plugIn인 TaintToleration을 disable 시켰고 custom plugIn은 enable tlzuTek.

### Multiple Profiles

- 둘 이상의 프로파일을 실행하도록 kube-scheduler를 구성할 수 있다.
- 여러 개의 kubeSchedulerConfiguration 객체를 정의할 수 있다.
- 하지만, 각각 다른 Process이기 때문에, 결국 관리를 위한 추가 노력이 필요하다.


## Admission Controllers

- 기존에는 kubectl -> Authentication -> Authorization -> create Pod 순으로 이루어짐
- request가 API 서버에 도달하면 인증 단계를 거치게 되는데 이는 인증서를 통해 이루어 진다.
- kube/config 파일에 인증서가 구성되어 있고 Authentication 프로세스가 이를 확인한다.
- 그다음 Authorization 단계를 거치게 되는데 이는 RoleBase 기반이다. 설정된 Role을 기반으로 엑세스 제어를 통제한다.

그러나 이러한 RoleBase 기반의 엑세스 제어로는 달성할 수 없는 부분이 있었고 이를 해결하기 위해 `Admission Controller`가 등장.

#### Admission Controllers

이는 단순이 요청의 유효성을 검사하는 것을 넘어 Pod가 생성되기전에 Request 자체를 변경하거나 추가 작업을 수행할 수 있다. 이는 PlugIn을 통해 구현된다.

예를 들어, 어떤 사용자가 k8s API를 통해 Pod 생성을 요청했을 때 해당 Pod의 설정이 적절한지 검증함으로써 Pod 템플릿을 변경할 수도 있고, 부적절하다고 판단되면 아예 요청을 거부할 수도 있다.  다음은 많이 쓰이는 예시이다.
- LimitRanger, ResourceQuota: Pod의 리소스 request, limit을 자동으로 설정해준다.
- lstio의 Envoy Proxy Injection : Ingection=true인 ns의 모든 Pod는 sidecar가 자동으로 추가된다.
- 심지어 ServiceAccount도 Admission Controller plugin의 한 종류이다.

이러한 작업을 수행하기 위해 `Admission Controllers`는 두 가지 단계를 거치도록 구현되어 있다. 하나는 `Mutate` 이고 다른 하나는 `Validate`  이다. 
- `Mutate`
  사용자가 요청한 API의 매니페스트를 검사해 적절한 값으로 수정하는 변형(Mutate) 작업을 수행한다.
- `Validate`
  매니페스트가 유효한지 검사(Validate)함으로써 요청을 거절할 것인지 결정한다.

![[Pasted image 20250317000310.png | 700]]
- API 서버로 들어오는 모든 요청은 최종적으로 `etcd`에 저장되는데 그 전에 Auth를 거치게 된다. JWT 또는 인증서 등을 통해 client를 인증한 뒤, client의 API 요청이 `RBAC` 권한과 매칭되는지를 검사한다. 
- Client가 해당 요청을 수행할 수 있는 적절한 권한이 있다고 판단되면, API 서버는 Admission Controller의 `Mutate`, `Validate` 과정을 거쳐 etcd에 요청 데이터를 저장한다. 그 뒤에는 컨트롤러나 스케줄러 등이 etcd의 데이터를 감지해 그들이 해야할 일을 수행한다.
- **API 서버는 `Mutate`와 `Validate` 과정을 거친 뒤에야 비로소 `etcd`에 요청 데이터를 저장한다.** 
- 특히, `Container`에 `root privileged`권한을 부여하는 것은 굉장히 많은 보안 취약점을 가져오기 때문에 `PodSecurityPolicy`라는 별도의 `Admission Controller`가 k8s에 이미 내장되어 있다.

이미 종류의 `Admission Controllers`가 기본적으로 활성화 되어 있다. 

`Admission Controllers` PlugIn을 추가하는 법은 간단하다.

![[Pasted image 20250317000828.png | 600]]

[참고 블로그](https://blog.naver.com/alice_k106/221546328906)
